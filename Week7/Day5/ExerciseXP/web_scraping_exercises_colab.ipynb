{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cf0b68",
   "metadata": {},
   "source": [
    "# ‚úÖ Web Scraping Exercises with BeautifulSoup\n",
    "Optimized for Google Colab\n",
    "Last Updated: July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13bef6",
   "metadata": {},
   "source": [
    "## üåü Exercise 1 : Parsing HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Sports World</title>\n",
    "</head>\n",
    "<body>\n",
    "    <p>Your one-stop destination for the latest sports news and videos.</p>\n",
    "    <a href=\"#football\">Football</a>\n",
    "    <a href=\"#basketball\">Basketball</a>\n",
    "    <a href=\"#tennis\">Tennis</a>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(\"Page Title:\", soup.title.string)\n",
    "print(\"Paragraphs:\", [p.text for p in soup.find_all(\"p\")])\n",
    "print(\"Links:\", [a[\"href\"] for a in soup.find_all(\"a\", href=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5601151",
   "metadata": {},
   "source": [
    "## üåü Exercise 2 : Scraping robots.txt from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "robots = urlopen(\"https://en.wikipedia.org/robots.txt\").read().decode(\"utf-8\")\n",
    "print(robots[:500])  # –ø–µ—á–∞—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382d355",
   "metadata": {},
   "source": [
    "## üåü Exercise 3 : Extracting Headers from Wikipedia‚Äôs Main Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_soup(url):\n",
    "    req = Request(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    return BeautifulSoup(urlopen(req), \"html.parser\")\n",
    "\n",
    "soup = get_soup(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "headers = [tag.text.strip() for tag in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])]\n",
    "print(headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3dc51d",
   "metadata": {},
   "source": [
    "## üåü Exercise 4 : Checking for Page Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec65fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup = get_soup(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "print(\"Page has a title:\", soup.title.string if soup.title and soup.title.string.strip() else \"No title found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf9374",
   "metadata": {},
   "source": [
    "## üåü Exercise 5 : Analyzing US-CERT Security Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd875828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "soup = get_soup(\"https://www.cisa.gov/news-events/cybersecurity-advisories\")\n",
    "current_year = str(datetime.now().year)\n",
    "alerts = [a.text.strip() for a in soup.find_all(\"a\") if current_year in a.text and (\"CSA\" in a.text or \"ICSA\" in a.text)]\n",
    "print(f\"Security alerts in {current_year}: {len(alerts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47211715",
   "metadata": {},
   "source": [
    "## üåü Exercise 6 : Scraping Movie Details from IMDB (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "soup = get_soup(\"https://www.imdb.com/chart/top/\")\n",
    "movies = soup.select(\"td.titleColumn\")\n",
    "links = soup.select(\"td.titleColumn a\")\n",
    "\n",
    "random_indices = random.sample(range(len(movies)), 10)\n",
    "\n",
    "for i in random_indices:\n",
    "    name = links[i].text\n",
    "    year = movies[i].span.text.strip(\"()\")\n",
    "    movie_url = \"https://www.imdb.com\" + links[i][\"href\"]\n",
    "    movie_soup = get_soup(movie_url)\n",
    "    summary_tag = movie_soup.select_one('[data-testid=\"plot-xl\"], [data-testid=\"plot-l\"], .sc-16ede01-2')\n",
    "    summary = summary_tag.text.strip() if summary_tag else \"No summary\"\n",
    "    print(f\"{name} ({year}): {summary}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
