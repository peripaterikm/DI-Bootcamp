{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f88b63d",
   "metadata": {},
   "source": [
    "# Web Scraping Exercises with BeautifulSoup\n",
    "Last Updated: September 26th, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c37c4",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 1 : Parsing HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example HTML (replace with urlopen if needed)\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Sports World</title>\n",
    "</head>\n",
    "<body>\n",
    "    <p>Your one-stop destination for the latest sports news and videos.</p>\n",
    "    <a href=\"#football\">Football</a>\n",
    "    <a href=\"#basketball\">Basketball</a>\n",
    "    <a href=\"#tennis\">Tennis</a>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Title\n",
    "print(\"Page Title:\", soup.title.string)\n",
    "\n",
    "# Paragraphs\n",
    "paragraphs = [p.text for p in soup.find_all(\"p\")]\n",
    "print(\"Paragraphs:\", paragraphs)\n",
    "\n",
    "# Links\n",
    "links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "print(\"Links:\", links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717e8aa",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 2 : Scraping robots.txt from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"https://en.wikipedia.org/robots.txt\"\n",
    "robots = urlopen(url).read().decode(\"utf-8\")\n",
    "print(robots)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35d839",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 3 : Extracting Headers from Wikipediaâ€™s Main Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7fa89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
    "\n",
    "headers = [tag.text.strip() for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])]\n",
    "print(headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d839788",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 4 : Checking for Page Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
    "\n",
    "if soup.title and soup.title.string.strip():\n",
    "    print(\"Page has a title:\", soup.title.string)\n",
    "else:\n",
    "    print(\"No title found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fe38e",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 5 : Analyzing US-CERT Security Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://www.cisa.gov/news-events/cybersecurity-advisories\"\n",
    "soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
    "\n",
    "current_year = str(datetime.now().year)\n",
    "alerts = [a.text for a in soup.find_all(\"a\") if current_year in a.text]\n",
    "\n",
    "print(f\"Security alerts in {current_year}: {len(alerts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f5ed1",
   "metadata": {},
   "source": [
    "## ðŸŒŸ Exercise 6 : Scraping Movie Details from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ab388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "url = \"https://www.imdb.com/chart/top/\"\n",
    "soup = BeautifulSoup(urlopen(url), \"html.parser\")\n",
    "\n",
    "movies = soup.select(\"td.titleColumn\")\n",
    "links = soup.select(\"td.titleColumn a\")\n",
    "\n",
    "random_indices = random.sample(range(len(movies)), 10)\n",
    "\n",
    "for i in random_indices:\n",
    "    name = links[i].text\n",
    "    year = movies[i].span.text.strip(\"()\")\n",
    "    movie_url = \"https://www.imdb.com\" + links[i][\"href\"]\n",
    "    movie_soup = BeautifulSoup(urlopen(movie_url), \"html.parser\")\n",
    "    summary_tag = movie_soup.find(\"span\", {\"data-testid\": \"plot-l\"})\n",
    "    summary = summary_tag.text if summary_tag else \"No summary\"\n",
    "    print(f\"{name} ({year}): {summary}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
